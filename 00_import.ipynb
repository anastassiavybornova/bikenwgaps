{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c7573ec",
   "metadata": {},
   "source": [
    "# DOC\n",
    "## LAST UPDATE: 2022-02-16\n",
    "## IPDC PROCEDURE - STEP 0: IMPORT DATA, CREATE & SIMPLIFY NETWORKS\n",
    "\n",
    "input: 4 csv files - data from OSM\n",
    "* './data/copenhagen_biketrack_edges.csv'\n",
    "* './data/copenhagen_carall_edges.csv'\n",
    "* './data/copenhagen_biketrack_nodes.csv'\n",
    "* './data/copenhagen_carall_nodes.csv'\n",
    "\n",
    "output: 9 pickle files - networks, conversion tables and betweenness centrality values\n",
    "* \"./data/pickle/mnw.gpickle\" (input network, non-simplified)\n",
    "* \"./data/pickle/mnwl.gpickle\" (input network largest connected component, non-simplified)\n",
    "H and B - whole NW (largest connected component) H and bikeable-only NW B as nx objects (simplified, lcc)\n",
    "* \"./data/pickle/H.gpickle\"\n",
    "* \"./data/pickle/B.gpickle\"\n",
    "h and b as igraph objects\n",
    "* \"./data/pickle/h.pickle\"\n",
    "* \"./data/pickle/b.pickle\"\n",
    "eids and nids conversion tables from igraph to nx\n",
    "* \"./data/pickle/eids_conv.pickle\"\n",
    "* \"./data/pickle/nids_conv.pickle\"\n",
    "betweenness centrality values\n",
    "* \"./data/pickle/ebc.pickle\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d33c6e9",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaac33b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "%run -i packages.py\n",
    "\n",
    "# CUSTOM FUNCTION\n",
    "\n",
    "# define function that creates attribute dictionary for nodes and edges\n",
    "# (for input to nx.add_edges_from/add_nodes_from)\n",
    "def make_attr_dict(*args, **kwargs): \n",
    "    \n",
    "    argCount = len(kwargs)\n",
    "    \n",
    "    if argCount > 0:\n",
    "        attributes = {}\n",
    "        for kwarg in kwargs:\n",
    "            attributes[kwarg] = kwargs.get(kwarg, None)\n",
    "        return attributes\n",
    "    else:\n",
    "        return None # (if no attributes are given)\n",
    "    \n",
    "# create subfolder to save pickle files as results\n",
    "os.mkdir(\"./data/pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87949ec",
   "metadata": {},
   "source": [
    "# DATA IMPORT AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60320f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anvy/opt/anaconda3/envs/bnwenv/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:122: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  arr = construct_1d_object_array_from_listlike(values)\n",
      "/Users/anvy/opt/anaconda3/envs/bnwenv/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:122: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  arr = construct_1d_object_array_from_listlike(values)\n",
      "/Users/anvy/opt/anaconda3/envs/bnwenv/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:122: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  arr = construct_1d_object_array_from_listlike(values)\n",
      "/Users/anvy/opt/anaconda3/envs/bnwenv/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:122: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  arr = construct_1d_object_array_from_listlike(values)\n"
     ]
    }
   ],
   "source": [
    "# bike edges\n",
    "be = pd.read_csv('./data/copenhagen_biketrack_edges.csv').drop(columns = [\"key\", \"lanes\", \"name\", \"highway\", \"maxspeed\", \"bridge\", \"tunnel\", \"junction\", \"width\", \"access\", \"ref\", \"service\", \"area\"])\n",
    "be[\"geometry\"] = be.apply(lambda x: shapely.wkt.loads(x.geometry), axis = 1)\n",
    "be = gpd.GeoDataFrame(be, geometry = \"geometry\") \n",
    "\n",
    "# car edges\n",
    "ce = pd.read_csv('./data/copenhagen_carall_edges.csv').drop(columns = [\"key\", \"lanes\", \"name\", \"highway\", \"maxspeed\", \"bridge\", \"tunnel\", \"junction\", \"width\", \"access\", \"ref\", \"service\"])\n",
    "ce[\"geometry\"] = ce.apply(lambda x: shapely.wkt.loads(x.geometry), axis = 1)\n",
    "ce = gpd.GeoDataFrame(ce, geometry = \"geometry\") \n",
    "\n",
    "# bike nodes\n",
    "bn = pd.read_csv('./data/copenhagen_biketrack_nodes.csv').drop(columns = [\"highway\", \"ref\"])\n",
    "bn[\"geometry\"] = bn.apply(lambda x: shapely.wkt.loads(x.geometry), axis = 1)\n",
    "bn = gpd.GeoDataFrame(bn, geometry = \"geometry\")\n",
    "\n",
    "# car nodes\n",
    "cn = pd.read_csv('./data/copenhagen_carall_nodes.csv').drop(columns = [\"highway\", \"ref\"])\n",
    "cn[\"geometry\"] = cn.apply(lambda x: shapely.wkt.loads(x.geometry), axis = 1)\n",
    "cn = gpd.GeoDataFrame(cn, geometry = \"geometry\")\n",
    "\n",
    "# AN: dataframe of ALL NODES\n",
    "\n",
    "# merge all nodes to one dataframe, \n",
    "an = pd.merge(bn, cn, how = \"outer\", indicator = True) # merging\n",
    "an[\"type\"] = an[\"_merge\"].cat.rename_categories([\"bike\", \"car\", \"multi\"]) # adding info on type\n",
    "an = an.drop(columns = \"_merge\")\n",
    "an = an.sort_values(by = \"osmid\").reset_index(drop = True) # sort by osmid\n",
    "# make attribute dictionary with type and geocoordinates for each node\n",
    "an[\"attr_dict\"] = an.apply(lambda x: make_attr_dict(category_node = x.type, coord = x.geometry), axis = 1) # add attr_dict\n",
    "\n",
    "# AE: dataframe of ALL EDGES\n",
    "\n",
    "# make df with all edges (ae) to pass it to nx\n",
    "\n",
    "# add edge ids (strings with \"id1, id2\" sorted (id1 < id2))\n",
    "be[\"edge_id\"] = be.apply(lambda x: str(sorted([x[\"u\"], x[\"v\"]])), axis = 1)\n",
    "ce[\"edge_id\"] = ce.apply(lambda x: str(sorted([x[\"u\"], x[\"v\"]])), axis = 1)\n",
    "# (edge ids are set as strings; converting back: with \"from ast import literal_eval\" fct)\n",
    "# finding duplicates by [\"osmid\", \"oneway\", \"edge_id\", \"length\"]\n",
    "\n",
    "# simplifying network into undirected - beu and ceu contain the \"undirected\" edges\n",
    "# (removing all parallel edges)\n",
    "\n",
    "beu = be.drop_duplicates(subset = [\"osmid\", \"oneway\", \"edge_id\", \"length\"],\n",
    "                  keep = \"first\",\n",
    "                  inplace = False,\n",
    "                  ignore_index = True).copy()\n",
    "ceu = ce.drop_duplicates(subset = [\"osmid\", \"oneway\", \"edge_id\", \"length\"],\n",
    "                  keep = \"first\",\n",
    "                  inplace = False,\n",
    "                  ignore_index = True).copy()\n",
    "\n",
    "# add type info prior to merging\n",
    "beu[\"type\"] = \"bike\"\n",
    "ceu[\"type\"] = \"car\"\n",
    "\n",
    "# concatenate\n",
    "ae = pd.concat([beu, ceu]).reset_index(drop = True)\n",
    "\n",
    "# change type for \"multi\" for edges that appear in both sets\n",
    "ae.loc[ae.duplicated(subset = [\"u\", \"v\", \"osmid\", \"oneway\", \"length\", \"edge_id\"], keep = False), \"type\"] = \"multi\"\n",
    "\n",
    "# remove duplicates\n",
    "ae = ae.drop_duplicates(subset = [\"u\", \"v\", \"osmid\", \"oneway\", \"length\", \"edge_id\", \"type\"], \n",
    "                          keep = \"first\",\n",
    "                          ignore_index = True, \n",
    "                          inplace = False)\n",
    "\n",
    "ae_tokeep = ae[ae.duplicated(\"edge_id\", keep = False) & (ae[\"type\"]==\"bike\")].index\n",
    "ae_todrop = ae[ae.duplicated(\"edge_id\", keep = False) & (ae[\"type\"] == \"car\")].index\n",
    "\n",
    "ae.loc[ae_tokeep, \"type\"] = \"multi\"\n",
    "ae = ae.drop(ae_todrop)\n",
    "\n",
    "# add attribute dictionary (for nx)\n",
    "ae[\"attr_dict\"] = ae.apply(lambda x: make_attr_dict(length = x.length, \n",
    "                                                    category_edge = x.type,\n",
    "                                                    edge_id = x.edge_id,\n",
    "                                                    coord = x.geometry,\n",
    "                                                    intnodes = []), # intnodes attribute: for storing simplification info on interstitial nodes \n",
    "                             axis = 1)\n",
    "\n",
    "# sort by \"left\" node (id1 < id2 - to control order of tuple keys in nx)\n",
    "ae[\"order\"] = ae.apply(lambda x: np.min([x[\"u\"], x[\"v\"]]), axis = 1)\n",
    "ae = ae.sort_values(by = \"order\").reset_index(drop = True)\n",
    "ae[\"orig\"] = ae.apply(lambda x: np.min([x[\"u\"], x[\"v\"]]), axis = 1)\n",
    "ae[\"dest\"] = ae.apply(lambda x: np.max([x[\"u\"], x[\"v\"]]), axis = 1)\n",
    "ae = ae.drop(columns = [\"order\", \"u\", \"v\"]) # instead of \"u\" and \"v\",\n",
    "# we will use \"origin\" and \"destination\" where osmid(origin) < osmid (destination)!\n",
    "\n",
    "del(ae_todrop, ae_tokeep, beu, ceu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b4b6a5",
   "metadata": {},
   "source": [
    "# MAKE NETWORK IN NETWORKX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e76b6243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of disconnected components on mnw: 77\n",
      "size of lcc: 38143\n"
     ]
    }
   ],
   "source": [
    "# CREATE NX OBJECTS\n",
    "\n",
    "# make multinetwork containing ALL edges\n",
    "mnw = nx.Graph()\n",
    "mnw.add_nodes_from(an.loc[:,[\"osmid\", \"attr_dict\"]].itertuples(index = False))\n",
    "mnw.add_edges_from(ae.loc[:,[\"orig\", \"dest\", \"attr_dict\"]].itertuples(index = False))\n",
    "\n",
    "# save to pickle (\"original\" nw = non-simplified, with disconnected components)\n",
    "nx.write_gpickle(mnw, \"./data/pickle/mnw.gpickle\")\n",
    "\n",
    "# KEEP ONLY LARGEST CONNECTED COMPONENT\n",
    "\n",
    "# make list of connected components\n",
    "cd_nodeset = []\n",
    "\n",
    "for comp in nx.connected_components(mnw):\n",
    "    \n",
    "    cd_nodeset = cd_nodeset + [comp]\n",
    "    \n",
    "n = len(cd_nodeset)\n",
    "    \n",
    "print(\"number of disconnected components on mnw: \" + str(n))\n",
    "\n",
    "cd_size = [None]*n\n",
    "cd_network = [None]*n\n",
    "cd_coord_dict = [None]*n\n",
    "cd_coord_list = [None]*n\n",
    "cd_types = [None]*n\n",
    "\n",
    "for i in range(n):\n",
    "    cd_size[i] = len(cd_nodeset[i])\n",
    "    cd_network[i] = nx.subgraph(mnw, cd_nodeset[i])\n",
    "    cd_coord_dict[i] = nx.get_edge_attributes(cd_network[i], \"coord\")\n",
    "    cd_coord_list[i] = [cd_coord_dict[i][key] for key in cd_coord_dict[i].keys()]\n",
    "    cd_types[i] = nx.get_edge_attributes(cd_network[i], \"category_edge\")\n",
    "\n",
    "# make df with info on connected components\n",
    "comps = pd.DataFrame({\n",
    "    'nodeset': cd_nodeset, \n",
    "    'size': cd_size,\n",
    "    'network': cd_network,\n",
    "    'coord': cd_coord_list,\n",
    "    'type': cd_types})\n",
    "\n",
    "del(cd_nodeset, cd_size, cd_network, cd_coord_list, cd_types, cd_coord_dict)\n",
    "\n",
    "# lcc is the size of the largest connected component\n",
    "lcc = np.max(comps[\"size\"])\n",
    "\n",
    "print(\"size of lcc: \" + str(lcc))\n",
    "\n",
    "comps = comps.sort_values(by = \"size\", ascending = False).reset_index(drop = True)\n",
    "\n",
    "# DEFINE MNWL as largest connected component\n",
    "# (drop all others)\n",
    "mnwl_nodes = comps[\"nodeset\"][0]\n",
    "mnwl_edges = ae.loc[ae.apply(lambda x: x.orig in mnwl_nodes, axis = 1),:].copy().reset_index(drop = True)\n",
    "mnwl = nx.subgraph(mnw, mnwl_nodes)\n",
    "\n",
    "# save as pickle (\"original\" nw = non-simplified, but only LCC)\n",
    "nx.write_gpickle(mnwl, \"./data/pickle/mnwl.gpickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541a517",
   "metadata": {},
   "source": [
    "# RUN SIMPLIFICATION ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2be1fd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1, Wed Feb 16 18:12:32 2022\n",
      "Run 2, Wed Feb 16 18:13:40 2022\n",
      "Run 3, Wed Feb 16 18:14:01 2022\n",
      "Run 4, Wed Feb 16 18:14:08 2022\n",
      "Run 5, Wed Feb 16 18:14:10 2022\n",
      "Run 6, Wed Feb 16 18:14:11 2022\n",
      "Run 7, Wed Feb 16 18:14:11 2022\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# make a copy of mnwl - H will be simplified and manipulated throughout while loop\n",
    "H = mnwl.copy()\n",
    "\n",
    "# set parameters for the while loop\n",
    "simplify_further = True\n",
    "run = 0\n",
    "\n",
    "# make dictionary of edge attributes of mnwl\n",
    "mnwl_typedict = nx.get_edge_attributes(mnwl, \"category_edge\")\n",
    "\n",
    "# loop runs while there are interstitial nodes on the nw\n",
    "while simplify_further:\n",
    "    \n",
    "    run += 1\n",
    "    print(\"Run \" + str(run) + \", \" + time.ctime())\n",
    "    \n",
    "    # get all nodes from nw\n",
    "    points_all_list = sorted(list(H.nodes))\n",
    "\n",
    "    # get all node degrees\n",
    "    degrees_all_list = [None]*len(points_all_list)\n",
    "    for i in range(len(points_all_list)):\n",
    "        degrees_all_list[i] = H.degree(points_all_list[i])\n",
    "\n",
    "    # make df with node + degree info + remove (T/F) + types (of incident edges)\n",
    "    pointsall = pd.DataFrame({\n",
    "        \"osmid\": points_all_list, \n",
    "        \"d\": degrees_all_list, \n",
    "        \"remove\": None, \n",
    "        \"types\": None})\n",
    "    \n",
    "    # get edge attributes (of CURRENT nw) as dict\n",
    "    catdict = nx.get_edge_attributes(H, \"category_edge\")\n",
    "    \n",
    "    # get edge type information (car/bike/multi) from attribute dictionary\n",
    "    pointsall[\"types\"] = pointsall.apply(lambda x: \n",
    "                                         [ catdict[tuple(sorted(edge))] for edge in H.edges(x.osmid) ], \n",
    "                                         axis = 1)\n",
    "\n",
    "    # split df in \"endpoints\" and d2 nodes\n",
    "    pointsend = pointsall[pointsall[\"d\"]!=2].copy().reset_index(drop = True)\n",
    "    pointsd2 = pointsall[pointsall[\"d\"]==2].copy().reset_index(drop = True)\n",
    "\n",
    "    # non-d2 nodes: all of them are remove=False (to keep)\n",
    "    pointsend[\"remove\"] = False\n",
    "    # d2 nodes: the ones that have same 2 edge types incident are remove=True\n",
    "    pointsd2[\"remove\"] = pointsd2.apply(lambda x: x.types[0]==x.types[1], axis = 1)\n",
    "\n",
    "    # final result: 2 dfs - nodes_final and nodes_interstitial\n",
    "\n",
    "    # nodes_final = nodes to keep (either they have d!=2 or they have d==2 but 2 different edge types)\n",
    "    nodes_final = pd.concat([pointsend, pointsd2[pointsd2[\"remove\"]==False].copy()]).reset_index(drop = True)\n",
    "\n",
    "    # nodes_interstitial = nodes to remove (d2 nodes with same 2 edge types incident)\n",
    "    nodes_interstitial = pointsd2[pointsd2[\"remove\"]==True].copy().reset_index(drop = True)\n",
    "    nodes_interstitial[\"types\"] = nodes_interstitial.apply(lambda x: x.types[0], axis = 1) # remove second-edge info (is same as first)\n",
    "\n",
    "    del(pointsall, catdict, degrees_all_list, points_all_list, pointsend, pointsd2)\n",
    "\n",
    "    # save info about endpoint/interstitial to node attributes on mnwl\n",
    "    for i in range(len(nodes_interstitial)):\n",
    "        H.nodes[nodes_interstitial.loc[i, \"osmid\"]][\"category_point\"] = \"int\"\n",
    "    for i in range(len(nodes_final)):\n",
    "        H.nodes[nodes_final.loc[i, \"osmid\"]][\"category_point\"] = \"end\"\n",
    "\n",
    "    # make df with interstitial edges\n",
    "    eint = nodes_interstitial.copy() \n",
    "    eint[\"orig\"] = eint.apply(lambda x: sorted([n for n in H.neighbors(x.osmid)])[0], axis = 1)\n",
    "    eint[\"dest\"] = eint.apply(lambda x: sorted([n for n in H.neighbors(x.osmid)])[1], axis = 1)\n",
    "\n",
    "    # add info on edge lengths\n",
    "    lendict = nx.get_edge_attributes(H, \"length\")\n",
    "    eint[\"length_new\"] = eint.apply(lambda x: \n",
    "                                    np.sum(\n",
    "                                        [lendict[tuple(sorted(edge))] for edge in H.edges(x.osmid)]\n",
    "                                    ), \n",
    "                                    axis = 1)\n",
    "\n",
    "    stack = list(np.unique(eint[\"osmid\"]))\n",
    "    \n",
    "    Hprior = H.copy() # make a copy of the nw in each simplification step\n",
    "    # to use for checking for neighbours for removing from stack\n",
    "    \n",
    "    # interstitial nodes dictionary - to keep track of nodes that are removed by \"while stack\"\n",
    "    intnodesdict = nx.get_edge_attributes(H, \"intnodes\")\n",
    "    # edge coordinate dictionary - to merge linestrings of aggregated edges\n",
    "    edgecoorddict = nx.get_edge_attributes(H, \"coord\")\n",
    "    \n",
    "    while stack:\n",
    "\n",
    "        mynode = stack.pop()\n",
    "        \n",
    "        for n in nx.neighbors(Hprior, mynode): # remove neighbors from ORIGINAL nw\n",
    "            if n in stack:\n",
    "                stack.remove(n)\n",
    "                #print(\"removed \"+ str(n))\n",
    "                \n",
    "        # u and v are the neighbors of \"mynode\"\n",
    "        u = eint.loc[eint[\"osmid\"]==mynode][\"orig\"].values[0]\n",
    "        v = eint.loc[eint[\"osmid\"]==mynode][\"dest\"].values[0]\n",
    "        \n",
    "        # counter (to break out of loop if it is not increased)\n",
    "        nodes_removed = 0\n",
    "        \n",
    "        if (u,v) not in H.edges: # only if neighbors are not neighbors themselves - \n",
    "            # to avoid roundabouts from disappearing\n",
    "            \n",
    "            # get info on interstitional nodes (for deriving edge coordinates later on)\n",
    "            myintnodes = [intnodesdict[tuple(sorted(edge))] for edge in H.edges(mynode)]\n",
    "            myintnodes.append([mynode])\n",
    "            myintnodes = [x for x in list(itertools.chain.from_iterable(myintnodes)) if x]\n",
    "            \n",
    "            H.add_edge(u_of_edge = u,\n",
    "                        v_of_edge = v,\n",
    "                        length = eint.loc[eint[\"osmid\"]==mynode][\"length_new\"].values[0],\n",
    "                        category_edge = eint.loc[eint[\"osmid\"]==mynode][\"types\"].values[0],\n",
    "                        intnodes = myintnodes,\n",
    "                        edge_id = str(sorted([u, v])),\n",
    "                        coord = shapely.ops.linemerge( [ edgecoorddict[tuple(sorted([u,mynode]))],\n",
    "                                                         edgecoorddict[tuple(sorted([v,mynode]))] ]\n",
    "                                                     ) \n",
    "                      )\n",
    "\n",
    "            H.remove_node(mynode)\n",
    "            nodes_removed += 1\n",
    "    \n",
    "    if nodes_removed == 0:\n",
    "        \n",
    "        simplify_further = False # to break out of loop\n",
    "                \n",
    "        # save simplified network to H gpickle\n",
    "        nx.write_gpickle(H, \"./data/pickle/H.gpickle\") \n",
    "        \n",
    "        print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2066d6aa",
   "metadata": {},
   "source": [
    "# MAKE \"BIKEABLE\" NETWORK, IGRAPH OBJECTS AND NX/IG CONVERSION TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23cb3a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make \"bikeable\" network from H (excluding car edges)\n",
    "bikeable_nodes = [node for node in H.nodes if H.nodes[node][\"category_node\"]!=\"car\"]\n",
    "H_noncar_induced = H.subgraph(bikeable_nodes).copy() \n",
    "# induced subgraph - still contains the car edges that lie between multi nodes; - exclude them:\n",
    "banw = H_noncar_induced.copy()\n",
    "banw.remove_edges_from([edge for edge in banw.edges if banw.edges[edge][\"category_edge\"]==\"car\"])\n",
    "nx.write_gpickle(banw, \"./data/pickle/B.gpickle\") \n",
    "\n",
    "# conversion to igraph\n",
    "h = ig.Graph.from_networkx(H)\n",
    "h.write_pickle(\"./data/pickle/h.pickle\")\n",
    "b = ig.Graph.from_networkx(banw)\n",
    "b.write_pickle(\"./data/pickle/b.pickle\")\n",
    "# to read in again: Graph.Read_Pickle()\n",
    "\n",
    "# eids: \"conversion table\" for edge ids from igraph to nx \n",
    "eids_nx = [tuple(sorted(literal_eval(h.es(i)[\"edge_id\"][0]))) for i in range(len(h.es))]\n",
    "eids_ig = [i for i in range(len(h.es))]\n",
    "eids_conv = pd.DataFrame({\"nx\": eids_nx, \"ig\": eids_ig})\n",
    "\n",
    "# nids: \"conversion table\" for node ids from igraph to nx\n",
    "nids_nx = [h.vs(i)[\"_nx_name\"][0] for i in range(len(h.vs))]\n",
    "nids_ig = [i for i in range(len(h.vs))]\n",
    "nids_conv = pd.DataFrame({\"nx\": nids_nx, \"ig\": nids_ig})\n",
    "\n",
    "eids_conv.to_pickle(\"./data/pickle/eids_conv.pickle\")\n",
    "nids_conv.to_pickle(\"./data/pickle/nids_conv.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b48bc0c",
   "metadata": {},
   "source": [
    "# COMPUTE EDGE BETWEENNESS CENTRALITY VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d5cc318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 s, sys: 34.5 ms, total: 20.1 s\n",
      "Wall time: 20 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# extract edge and node attributes as dictionaries\n",
    "\n",
    "tnd = nx.get_node_attributes(H, \"category_node\") # type of nodes dictionary tnd\n",
    "ted = nx.get_edge_attributes(H, \"category_edge\") # type of edges dictionary tnd\n",
    "led = nx.get_edge_attributes(H, \"length\") # length of edges dictionary led\n",
    "cnd = nx.get_node_attributes(H, \"coord\") # coordinates of nodes dictionary cnd\n",
    "ced = nx.get_edge_attributes(H, \"coord\") # coordinates of edges dictionary ced\n",
    "\n",
    "# make data frame of ebc with:\n",
    "ebc = pd.DataFrame({\"edge_ig\": [e.index for e in h.es]}) # igraph edge ID\n",
    "ebc[\"edge_nx\"] = ebc.apply(lambda x: tuple(literal_eval(h.es[x.edge_ig][\"edge_id\"])), axis = 1) # nx edge ID\n",
    "ebc[\"length\"] = ebc.apply(lambda x: h.es[x.edge_ig][\"length\"], axis = 1) # length in meters\n",
    "\n",
    "# compute ebcs:\n",
    "ebc[\"ebc_inf\"] = h.edge_betweenness(directed = False, cutoff = None, weights = \"length\") # \"standard\" ebc\n",
    "ebc[\"ebc_lambda\"] = h.edge_betweenness(directed = False, cutoff = 2500, weights = \"length\") # ebc only including *paths* below 2500m\n",
    "ebc.to_pickle(\"./data/pickle/ebc.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea7b6bd",
   "metadata": {},
   "source": [
    "### END OF 00_import NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnwker",
   "language": "python",
   "name": "bnwker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
